{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AneRBC-I Binary Classification (Anemic vs Healthy) with VGG16\n",
    "\n",
    "This notebook implements a binary classification pipeline using **AneRBC-I `Original_images` only**, designed to run in **Google Colab**.\n",
    "\n",
    "## Cases Compared\n",
    "- **Case 1**: VGG16 from scratch (`weights=None`, trainable base)\n",
    "- **Case 2**: Transfer learning (`weights='imagenet'`, frozen base only)\n",
    "\n",
    "## Fixed Split (seed=42)\n",
    "- Train: 700 images (350 Healthy / 350 Anemic)\n",
    "- Validation: 100 images (50 Healthy / 50 Anemic)\n",
    "- Test: 200 images (100 Healthy / 100 Anemic)\n",
    "\n",
    "## Dataset path (Google Drive)\n",
    "`/content/drive/MyDrive/Anemia FYP/AneRBC_dataset`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "\n",
    "1. Upload the dataset to Google Drive at: `My Drive/Anemia FYP/AneRBC_dataset`\n",
    "2. Run the first code cell — it will prompt you to authorise Google Drive access.\n",
    "3. Run all remaining cells in order.\n",
    "\n",
    "> Artifacts (model checkpoints, CSVs, plots) are saved back to your Drive under `My Drive/Anemia FYP/Code/ImageClassification/artifacts/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805b6d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Contents of /content/drive/:\n",
      "['Untitled spreadsheet (12).gsheet', 'Bank Transactions.gsheet', 'Which i only show off , not learning.gdoc', 'Untitled document (4).gdoc', 'Medication - Appaji & Sony.pdf', 'Medication - Appaji & Sony.gsheet', 'Web 3.0   Meta   What not.gdoc', 'How does culture influence language?.gdoc', 'Untitled spreadsheet (11).gsheet', 'KCET opiton entry.gdoc', 'Classroom', 'Crypto', 'Netflix  - Innovation studies.gdoc', 'Untitled document (3).gdoc', 'Untitled document (2).gdoc', 'Untitled presentation (1).gslides', 'To do.gdoc', 'Self i.gdoc', 'Indian Energy Sector.gdoc', 'Capstone Projects.gdoc', 'Untitled spreadsheet (10).gsheet', 'Digital Note Taking.gdoc', 'Projects.gdoc', 'Untitled spreadsheet (9).gsheet', 'Lata -.gdoc', 'CET Marks.gsheet', 'python.gdoc', 'Kriti 2022', 'Copy of OSSU CS Timeline .gsheet', 'Lost Indian Passport and American Green card in Paris (1).gdoc', 'Lost Indian Passport and American Green card in Paris.gdoc', 'Documents', 'Portfolio .gsheet', 'Artificial Intelligence - AI.gdoc', 'Git and GitHub.gdoc', 'Eng_2021_r3 fool.gsheet', 'Kcet Counselling website.gdoc', 'KCET', 'med_r2_2021.gsheet', 'Trip expenses.gsheet', 'IMG_20211031_141928058.jpg', 'IMG_20211031_142201221_HDR.jpg', 'Kishan KCET details.gdoc', \"❤️de's RV.gsheet\", 'Play Books Notes', 'Untitled spreadsheet (8).gsheet', 'Kiran - Letter to the hostel warden.gdoc', 'Things for hostel.gsheet', 'Untitled spreadsheet (7).gsheet', 'Affirmations -.gdoc', 'Hostel Mess fee refund.gdoc', 'Gym.gdoc', 'Tasks.gsheet', 'Kiran resume (1).pdf', 'Kiran resume.pdf', 'IMG-20221228-WA0017.jpg', 'Internals.gsheet', 'CGPA.gsheet', 'To Pass.gsheet', 'Why do we need a server .gdoc', 'Untitled document (1).gdoc', 'Communication Skills - English.gdoc', 'Me!.gdoc', 'IMG-20230220-WA0003.jpg', 'UPI.gdoc', 'UPI presentation.pptx', 'UPI Presentation.gdoc', 'UPI Report.gdoc', 'Goal Systems.gdoc', 'What I do.gdoc', 'Hostel issues - Letter.gdoc', 'My JSS marks.gsheet', 'Ai - ML.gdoc', 'Untitled spreadsheet (6).gsheet', 'Untitled spreadsheet (5).gsheet', 'Mysore Trip Expenses - JUL 2023.gsheet', 'IMG-20230727-WA0008.jpg', 'Mobile Phones.gsheet', 'Colab Notebooks', 'Anushka.gsheet', 'Study plan.gdoc', 'Kiran SOP.gdoc', 'Monthly Expenses.gsheet', 'Laptop.gsheet', 'To-do list.gsheet', \"This image has a list of medicines , give it's generic name in a tabular column.gsheet\", \"This image has a list of medicines , give all medicines along with it's generic name in a tabular column .gsheet\", 'Ration.gsheet', 'Monthly reports - Lata.gsheet', 'Things while traveling .gdoc', 'Discipline .gdoc', 'Untitled spreadsheet (4).gsheet', 'Laptop repair.gsheet', 'Untitled presentation.gslides', 'Varanasi .gdoc', 'Anushka .gdoc', 'UHV.gdoc', 'Untitled document.gdoc', 'DMS.gdoc', 'Ayodha expenses .gsheet', \"Prajwal's (PP) Donations.gsheet\", 'Kiran - Medications .gdoc', 'CSK vs RCB.gdoc', 'Money Sources.gdoc', 'Vehicles.gsheet', 'Kiran Resume JPMC.pdf', 'WEB DevelopMEnt.gdoc', 'Anushka - Germany.gsheet', 'B.gsheet', 'JPMC.gdoc', 'JPMC - interview:.gdoc', 'Envi.gdoc', 'E-waste (group-1)-1.pptx', 'E-waste (group-1).pptx', 'NetBanking.gdoc', 'Real Time Traffic Signal.gdoc', 'images.jpeg', 'Copy of Standard Resume Template.docx', 'Untitled form.gform', 'Kiran_N_resume_Sept_2024.pdf', 'Family_Drama', 'ISE marks.gsheet', 'Kiran_N_resume_Sept_2024-1.pdf', 'Thota - Hola.gsheet', 'My Placement.gsheet', 'Freshers day .gsheet', \"Physician and APP schedule reveal Jan-Jun '25.xlsx\", 'ENV (3).docx', 'ENV (2).docx', 'ENV (1).docx', 'Untitled spreadsheet (3).gsheet', 'dbms-1.pdf', 'IMG-20241201-WA0025.jpg', 'Kiran_resume_december (2).pdf', 'Kiran_resume_december (1).pdf', '01JST21IS020_Kiran.pdf', 'Kiran_resume_december.pdf', 'placement_portal.gdoc', 'kiran 4th sem results (1).pdf', 'kiran 4th sem results.pdf', 'kiran 3rd sem results.pdf', 'kiran 2nd sem results.pdf', 'kiran 1st sem results.pdf', 'Kiran SSLC Marks Card.pdf', 'Kiran PU Marks Card.pdf', 'Kiran e-aadhaar updated (1).pdf', 'Kiran e-aadhaar updated.pdf', 'Kiran Passport size Photo.jpg', 'Sample front sheet.gdoc', 'Sample front sheet.docx', 'Docs', 'Useful Code.gdoc', 'Untitled spreadsheet (2).gsheet', 'Screenshot_20250512-105133.png', 'KS10126479.pdf', 'KS10126479 (1).gdoc', 'KS10126479.gdoc', 'Certificates', 'pg_student_data_2026.csv', '2026 Placements', '01JST21IS020_Kiran_N.pdf', '01JST21IS020_NDA.pdf', 'PXL_20250616_002444162.jpg', 'Kiran_resume.pdf', 'Kiran_Placements', 'ENV.docx', 'Cisco Campus Visit 2026.gsheet', 'Sabre Absentees.gsheet', 'Unfit to travel.gdoc', 'MiniProject Notes.gdoc', 'Inventory Assignment.gdoc', 'Kiran_Morgan_Stanley.jpg', 'Cloud Computing.gdoc', 'Compnay_HR_Details.gsheet', 'Deekshith medical letter.gdoc', 'Deekshith medical letter EC OE.gdoc', 'Untitled spreadsheet (1).gsheet', 'Portal Juniors.gsheet', 'Placement Challenges and RoadMap.gdoc', 'UDgama Sponsorships.gsheet', 'Cross-College companies info.xlsx', 'Morgan Stanley.gdoc', 'Hostel Seat Discontinue Letter.gdoc', 'Money old.gsheet', 'Goals .gdoc', 'Untitled spreadsheet.gsheet', 'Udgama Hackathon Rubricks.gdoc', 'kiran_6th_sem_results.jpg', 'udgama.gsheet', 'kiran_internship.gslides', 'Persona Panel allotment.gsheet', 'Stocks.gdoc', 'Investment.gsheet', 'FYP.gdoc', 'Money.gsheet', 'AneRBC Dataset Structure.gslides', 'AneRBC Complete Deck (1).gslides', 'AneRBC Complete Deck.gslides', 'Internals +PYQs - 7th sem.gdoc', 'letter.pdf', 'Sheets.gsheet', 'Car Insurance Renewal Checklist and Comparison.gsheet', 'Car', 'Saved from Chrome', 'Screenshot_20260127-165023.png', 'Anemia FYP', 'LOA , Passport.gdoc', 'Saree Shop.gdoc', 'Saree Shop Quotation.gsheet']\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Print the contents of the mounted drive\n",
    "import os\n",
    "print(\"Contents of /content/drive/:\")\n",
    "print(os.listdir('/content/drive/MyDrive/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4988470d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip already exists at /content/drive/MyDrive/Anemia FYP/AneRBC_dataset.zip. Skipping download.\n",
      "Extracting dataset...\n",
      "Dataset extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the target directory in Google Drive\n",
    "target_dir = Path('/content/drive/MyDrive/Anemia FYP')\n",
    "target_dir.mkdir(parents=True, exist_ok=True)\n",
    "dataset_dir = target_dir / 'AneRBC_dataset'\n",
    "dataset_zip = target_dir / 'AneRBC_dataset.zip'\n",
    "\n",
    "# If the dataset folder already exists and is non-empty, skip everything\n",
    "if dataset_dir.exists() and any(dataset_dir.iterdir()):\n",
    "    print(f'Dataset already exists at {dataset_dir}. Skipping.')\n",
    "else:\n",
    "    if not dataset_zip.exists():\n",
    "        print('Downloading dataset...')\n",
    "        import urllib.request\n",
    "        url = 'https://data.mendeley.com/public-api/zip/hms3sjzt7f/download/1'\n",
    "        urllib.request.urlretrieve(url, dataset_zip)\n",
    "        print('Dataset downloaded successfully!')\n",
    "    else:\n",
    "        print(f'Zip already exists at {dataset_zip}. Skipping download.')\n",
    "\n",
    "    print('Extracting dataset...')\n",
    "    with zipfile.ZipFile(dataset_zip, 'r') as zf:\n",
    "        zf.extractall(target_dir)\n",
    "    print('Dataset extracted successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d51123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Project root: /content/drive/MyDrive/Anemia FYP\n",
      "Artifacts dir: /content/drive/MyDrive/Anemia FYP/Code/ImageClassification/artifacts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "SEED = 42\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "MAX_EPOCHS = 30\n",
    "LABEL_TO_ID = {\"Healthy\": 0, \"Anemic\": 1}\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "try:\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/Anemia FYP\")\n",
    "DATASET_ROOT = PROJECT_ROOT / \"AneRBC_dataset\" / \"AneRBC-I\"\n",
    "ANEMIC_DIR = DATASET_ROOT / \"Anemic_individuals\" / \"Original_images\"\n",
    "HEALTHY_DIR = DATASET_ROOT / \"Healthy_individuals\" / \"Original_images\"\n",
    "\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"Code\" / \"ImageClassification\" / \"artifacts\"\n",
    "SPLITS_DIR = ARTIFACTS_DIR / \"splits\"\n",
    "MODELS_DIR = ARTIFACTS_DIR / \"models\"\n",
    "METRICS_DIR = ARTIFACTS_DIR / \"metrics\"\n",
    "PLOTS_DIR = ARTIFACTS_DIR / \"plots\"\n",
    "\n",
    "for directory in [ARTIFACTS_DIR, SPLITS_DIR, MODELS_DIR, METRICS_DIR, PLOTS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_CSV = SPLITS_DIR / \"train_split.csv\"\n",
    "VAL_CSV = SPLITS_DIR / \"val_split.csv\"\n",
    "TEST_CSV = SPLITS_DIR / \"test_split.csv\"\n",
    "\n",
    "SCRATCH_CKPT = MODELS_DIR / \"vgg16_scratch_best.keras\"\n",
    "TRANSFER_CKPT = MODELS_DIR / \"vgg16_transfer_frozen_best.keras\"\n",
    "\n",
    "COMPARISON_CSV = METRICS_DIR / \"comparison_metrics.csv\"\n",
    "\n",
    "TRAINING_CURVES_PNG = PLOTS_DIR / \"training_curves.png\"\n",
    "CONFUSION_MATRICES_PNG = PLOTS_DIR / \"confusion_matrices.png\"\n",
    "ACCURACY_F1_PNG = PLOTS_DIR / \"accuracy_f1_comparison.png\"\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Artifacts dir:\", ARTIFACTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4884fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_EXTENSIONS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "\n",
    "def _collect_image_paths(folder: Path) -> list[Path]:\n",
    "    return sorted(\n",
    "        [p for p in folder.iterdir() if p.is_file() and p.suffix.lower() in VALID_EXTENSIONS],\n",
    "        key=lambda p: p.name,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_dataframe(anemic_dir: str, healthy_dir: str) -> pd.DataFrame:\n",
    "    anemic_paths = _collect_image_paths(Path(anemic_dir))\n",
    "    healthy_paths = _collect_image_paths(Path(healthy_dir))\n",
    "\n",
    "    rows = []\n",
    "    for path in healthy_paths:\n",
    "        rows.append({\"filepath\": str(path.resolve().relative_to(PROJECT_ROOT)), \"label\": \"Healthy\"})\n",
    "    for path in anemic_paths:\n",
    "        rows.append({\"filepath\": str(path.resolve().relative_to(PROJECT_ROOT)), \"label\": \"Anemic\"})\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"filepath\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def resolve_paths(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return a copy of df with filepaths resolved to absolute paths from PROJECT_ROOT.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"filepath\"] = df[\"filepath\"].apply(lambda p: str(PROJECT_ROOT / p))\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_splits(df: pd.DataFrame, seed: int = 42) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    stratify_labels = df[\"label\"].map(LABEL_TO_ID)\n",
    "\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.30,\n",
    "        random_state=seed,\n",
    "        shuffle=True,\n",
    "        stratify=stratify_labels,\n",
    "    )\n",
    "\n",
    "    temp_stratify_labels = temp_df[\"label\"].map(LABEL_TO_ID)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=2 / 3,\n",
    "        random_state=seed,\n",
    "        shuffle=True,\n",
    "        stratify=temp_stratify_labels,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train_df.reset_index(drop=True),\n",
    "        val_df.reset_index(drop=True),\n",
    "        test_df.reset_index(drop=True),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "700aa201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split files:\n",
      " - /content/drive/MyDrive/Anemia FYP/Code/ImageClassification/artifacts/splits/train_split.csv\n",
      " - /content/drive/MyDrive/Anemia FYP/Code/ImageClassification/artifacts/splits/val_split.csv\n",
      " - /content/drive/MyDrive/Anemia FYP/Code/ImageClassification/artifacts/splits/test_split.csv\n",
      "\n",
      "Split summary:\n",
      "   split  count\n",
      "0  train    700\n",
      "1    val    100\n",
      "2   test    200\n",
      "\n",
      "Per-class counts:\n",
      "train\n",
      "label\n",
      "Anemic     350\n",
      "Healthy    350\n",
      "Name: count, dtype: int64\n",
      "\n",
      "val\n",
      "label\n",
      "Anemic     50\n",
      "Healthy    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "test\n",
      "label\n",
      "Anemic     100\n",
      "Healthy    100\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def assert_split_integrity(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "    # Exact sample counts\n",
    "    assert len(train_df) == 700, f\"Expected 700 train samples, found {len(train_df)}\"\n",
    "    assert len(val_df) == 100, f\"Expected 100 val samples, found {len(val_df)}\"\n",
    "    assert len(test_df) == 200, f\"Expected 200 test samples, found {len(test_df)}\"\n",
    "\n",
    "    # Exact per-class counts\n",
    "    expected_class_counts = {\n",
    "        \"train\": {\"Healthy\": 350, \"Anemic\": 350},\n",
    "        \"val\": {\"Healthy\": 50, \"Anemic\": 50},\n",
    "        \"test\": {\"Healthy\": 100, \"Anemic\": 100},\n",
    "    }\n",
    "\n",
    "    split_map = {\"train\": train_df, \"val\": val_df, \"test\": test_df}\n",
    "    for split_name, split_df in split_map.items():\n",
    "        counts = split_df[\"label\"].value_counts().to_dict()\n",
    "        assert counts == expected_class_counts[split_name], (\n",
    "            f\"{split_name} class counts mismatch. Expected {expected_class_counts[split_name]}, got {counts}\"\n",
    "        )\n",
    "\n",
    "    # Zero overlap among filepaths (compare as relative paths)\n",
    "    train_paths = set(train_df[\"filepath\"])\n",
    "    val_paths = set(val_df[\"filepath\"])\n",
    "    test_paths = set(test_df[\"filepath\"])\n",
    "\n",
    "    assert train_paths.isdisjoint(val_paths), \"Leakage detected between train and val\"\n",
    "    assert train_paths.isdisjoint(test_paths), \"Leakage detected between train and test\"\n",
    "    assert val_paths.isdisjoint(test_paths), \"Leakage detected between val and test\"\n",
    "\n",
    "    # All files exist and are readable (resolve relative paths against PROJECT_ROOT)\n",
    "    all_paths = train_paths | val_paths | test_paths\n",
    "    bad_paths = [\n",
    "        path for path in all_paths\n",
    "        if not (PROJECT_ROOT / path).is_file() or not os.access(str(PROJECT_ROOT / path), os.R_OK)\n",
    "    ]\n",
    "    assert not bad_paths, f\"Found missing/unreadable files: {len(bad_paths)}\"\n",
    "\n",
    "\n",
    "full_df = build_dataframe(str(ANEMIC_DIR), str(HEALTHY_DIR))\n",
    "assert full_df.shape[0] == 1000, f\"Expected 1000 rows, found {full_df.shape[0]}\"\n",
    "\n",
    "train_df, val_df, test_df = make_splits(full_df, seed=SEED)\n",
    "assert_split_integrity(train_df, val_df, test_df)\n",
    "\n",
    "# Reproducibility check: rerunning with the same seed yields identical split memberships\n",
    "train_df_2, val_df_2, test_df_2 = make_splits(full_df, seed=SEED)\n",
    "assert set(train_df[\"filepath\"]) == set(train_df_2[\"filepath\"])\n",
    "assert set(val_df[\"filepath\"]) == set(val_df_2[\"filepath\"])\n",
    "assert set(test_df[\"filepath\"]) == set(test_df_2[\"filepath\"])\n",
    "\n",
    "# Stable output ordering\n",
    "train_df = train_df.sort_values(\"filepath\").reset_index(drop=True)\n",
    "val_df = val_df.sort_values(\"filepath\").reset_index(drop=True)\n",
    "test_df = test_df.sort_values(\"filepath\").reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv(TRAIN_CSV, index=False)\n",
    "val_df.to_csv(VAL_CSV, index=False)\n",
    "test_df.to_csv(TEST_CSV, index=False)\n",
    "\n",
    "print(\"Saved split files:\")\n",
    "print(\" -\", TRAIN_CSV)\n",
    "print(\" -\", VAL_CSV)\n",
    "print(\" -\", TEST_CSV)\n",
    "\n",
    "print(\"\\nSplit summary:\")\n",
    "summary_df = pd.DataFrame({\n",
    "    \"split\": [\"train\", \"val\", \"test\"],\n",
    "    \"count\": [len(train_df), len(val_df), len(test_df)],\n",
    "})\n",
    "print(summary_df)\n",
    "\n",
    "print(\"\\nPer-class counts:\")\n",
    "for split_name, split_df in {\"train\": train_df, \"val\": val_df, \"test\": test_df}.items():\n",
    "    print(split_name)\n",
    "    print(split_df[\"label\"].value_counts().sort_index())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f64e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 700 validated image filenames belonging to 2 classes.\n",
      "Found 100 validated image filenames belonging to 2 classes.\n",
      "Found 200 validated image filenames belonging to 2 classes.\n",
      "Generator checks passed.\n",
      "Class indices: {'Healthy': 0, 'Anemic': 1}\n",
      "Sample batch shape: (16, 224, 224, 3) Label shape: (16,)\n",
      "Pixel range: 0.04313725605607033 to 0.9568628072738647\n"
     ]
    }
   ],
   "source": [
    "def make_generators(train_df, val_df, test_df, image_size=(224, 224), batch_size=16):\n",
    "    classes = [\"Healthy\", \"Anemic\"]\n",
    "\n",
    "    # Resolve relative paths to absolute before passing to Keras generators\n",
    "    train_df_abs = resolve_paths(train_df)\n",
    "    val_df_abs = resolve_paths(val_df)\n",
    "    test_df_abs = resolve_paths(test_df)\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "    eval_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "    common_kwargs = {\n",
    "        \"x_col\": \"filepath\",\n",
    "        \"y_col\": \"label\",\n",
    "        \"target_size\": image_size,\n",
    "        \"color_mode\": \"rgb\",\n",
    "        \"class_mode\": \"binary\",\n",
    "        \"classes\": classes,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"validate_filenames\": True,\n",
    "    }\n",
    "\n",
    "    train_gen = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df_abs,\n",
    "        shuffle=True,\n",
    "        seed=SEED,\n",
    "        **common_kwargs,\n",
    "    )\n",
    "\n",
    "    val_gen = eval_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df_abs,\n",
    "        shuffle=False,\n",
    "        **common_kwargs,\n",
    "    )\n",
    "\n",
    "    test_gen = eval_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df_abs,\n",
    "        shuffle=False,\n",
    "        **common_kwargs,\n",
    "    )\n",
    "\n",
    "    return train_gen, val_gen, test_gen\n",
    "\n",
    "\n",
    "train_gen, val_gen, test_gen = make_generators(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    test_df,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Label encoding test\n",
    "expected_class_indices = {\"Healthy\": 0, \"Anemic\": 1}\n",
    "assert train_gen.class_indices == expected_class_indices, (\n",
    "    f\"Class indices mismatch. Expected {expected_class_indices}, got {train_gen.class_indices}\"\n",
    ")\n",
    "\n",
    "# Input shape/range test\n",
    "x_batch, y_batch = next(train_gen)\n",
    "assert x_batch.shape[1:] == (224, 224, 3), f\"Unexpected input shape: {x_batch.shape}\"\n",
    "assert np.min(x_batch) >= 0.0 and np.max(x_batch) <= 1.0, \"Input pixel range should be [0, 1]\"\n",
    "assert y_batch.ndim == 1, f\"Expected binary labels as rank-1 vector, got shape {y_batch.shape}\"\n",
    "train_gen.reset()\n",
    "\n",
    "print(\"Generator checks passed.\")\n",
    "print(\"Class indices:\", train_gen.class_indices)\n",
    "print(\"Sample batch shape:\", x_batch.shape, \"Label shape:\", y_batch.shape)\n",
    "print(\"Pixel range:\", float(np.min(x_batch)), \"to\", float(np.max(x_batch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "487f0308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "Model configuration checks passed.\n",
      "Case 1 base trainable: True\n",
      "Case 2 base trainable: False\n"
     ]
    }
   ],
   "source": [
    "def build_vgg16_binary(case: str) -> tf.keras.Model:\n",
    "    if case not in {\"scratch\", \"transfer_frozen\"}:\n",
    "        raise ValueError(\"case must be one of {'scratch', 'transfer_frozen'}\")\n",
    "\n",
    "    weights = None if case == \"scratch\" else \"imagenet\"\n",
    "    base = VGG16(\n",
    "        weights=weights,\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3),\n",
    "    )\n",
    "    base.trainable = case == \"scratch\"\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    x = base(inputs, training=base.trainable)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=f\"vgg16_{case}_binary\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Model configuration tests\n",
    "scratch_model = build_vgg16_binary(\"scratch\")\n",
    "transfer_model = build_vgg16_binary(\"transfer_frozen\")\n",
    "\n",
    "assert scratch_model.get_layer(\"vgg16\").trainable is True, \"Case 1 base should be trainable\"\n",
    "assert transfer_model.get_layer(\"vgg16\").trainable is False, \"Case 2 base should be frozen\"\n",
    "\n",
    "print(\"Model configuration checks passed.\")\n",
    "print(\"Case 1 base trainable:\", scratch_model.get_layer(\"vgg16\").trainable)\n",
    "print(\"Case 2 base trainable:\", transfer_model.get_layer(\"vgg16\").trainable)\n",
    "\n",
    "del scratch_model, transfer_model\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ab29293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Case 1: VGG16 without transfer learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m 3/44\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27:08\u001b[0m 40s/step - accuracy: 0.5174 - loss: 0.6935"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1270767205.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mtrained_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoint\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mhistories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1270767205.py\u001b[0m in \u001b[0;36mtrain_case\u001b[0;34m(case, train_gen, val_gen, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     ]\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_case(case: str, train_gen, val_gen, epochs: int = 300):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_vgg16_binary(case)\n",
    "\n",
    "    checkpoint_path = SCRATCH_CKPT if case == \"scratch\" else TRANSFER_CKPT\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=300,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(checkpoint_path),\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return model, history, checkpoint_path\n",
    "\n",
    "\n",
    "cases = [\n",
    "    (\"scratch\", \"Case 1: VGG16 without transfer learning\"),\n",
    "    (\"transfer_frozen\", \"Case 2: VGG16 transfer learning (frozen base)\"),\n",
    "]\n",
    "\n",
    "trained_models = {}\n",
    "histories = {}\n",
    "\n",
    "for case, description in cases:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(description)\n",
    "    model, history, checkpoint_path = train_case(case, train_gen, val_gen, epochs=MAX_EPOCHS)\n",
    "    trained_models[case] = {\"model\": model, \"checkpoint\": checkpoint_path}\n",
    "    histories[case] = history.history\n",
    "    print(f\"Saved best checkpoint to: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4dc1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_labels(model: tf.keras.Model, test_gen):\n",
    "    test_gen.reset()\n",
    "    y_true = test_gen.classes.astype(int)\n",
    "    y_prob = model.predict(test_gen, verbose=0).ravel()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    return y_true, y_pred, y_prob\n",
    "\n",
    "\n",
    "def evaluate_case(model, test_gen) -> dict[str, float]:\n",
    "    y_true, y_pred, _ = _predict_labels(model, test_gen)\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, pos_label=1)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, pos_label=1, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, pos_label=1, zero_division=0)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41bad7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for case, description in cases:\n",
    "    checkpoint_path = SCRATCH_CKPT if case == \"scratch\" else TRANSFER_CKPT\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "    best_model = tf.keras.models.load_model(checkpoint_path)\n",
    "    metrics = evaluate_case(best_model, test_gen)\n",
    "    y_true, y_pred, _ = _predict_labels(best_model, test_gen)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=[0, 1],\n",
    "        target_names=[\"Healthy\", \"Anemic\"],\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    # Evaluation tests\n",
    "    assert cm.shape == (2, 2), f\"Confusion matrix for {case} must be 2x2, got {cm.shape}\"\n",
    "    assert np.all(np.isfinite(list(metrics.values()))), f\"Non-finite metric values detected for {case}\"\n",
    "\n",
    "    results[case] = {\n",
    "        **metrics,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": report,\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(description)\n",
    "    print(\"Test metrics:\")\n",
    "    print({k: round(v, 4) for k, v in metrics.items()})\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be31b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_name_map = {\n",
    "    \"scratch\": \"Case 1\",\n",
    "    \"transfer_frozen\": \"Case 2\",\n",
    "}\n",
    "\n",
    "comparison_rows = []\n",
    "for case, _ in cases:\n",
    "    comparison_rows.append(\n",
    "        {\n",
    "            \"case\": case_name_map[case],\n",
    "            \"config\": case,\n",
    "            \"accuracy\": results[case][\"accuracy\"],\n",
    "            \"f1\": results[case][\"f1\"],\n",
    "            \"precision\": results[case][\"precision\"],\n",
    "            \"recall\": results[case][\"recall\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "comparison_df = comparison_df.sort_values(by=[\"accuracy\", \"f1\"], ascending=[False, False]).reset_index(drop=True)\n",
    "comparison_df.insert(0, \"rank\", np.arange(1, len(comparison_df) + 1))\n",
    "\n",
    "comparison_df.to_csv(COMPARISON_CSV, index=False)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves per case\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for row_idx, (case, description) in enumerate(cases):\n",
    "    history = histories[case]\n",
    "\n",
    "    axes[row_idx, 0].plot(history.get(\"loss\", []), label=\"train_loss\")\n",
    "    axes[row_idx, 0].plot(history.get(\"val_loss\", []), label=\"val_loss\")\n",
    "    axes[row_idx, 0].set_title(f\"{description} - Loss\")\n",
    "    axes[row_idx, 0].set_xlabel(\"Epoch\")\n",
    "    axes[row_idx, 0].set_ylabel(\"Loss\")\n",
    "    axes[row_idx, 0].legend()\n",
    "\n",
    "    axes[row_idx, 1].plot(history.get(\"accuracy\", []), label=\"train_accuracy\")\n",
    "    axes[row_idx, 1].plot(history.get(\"val_accuracy\", []), label=\"val_accuracy\")\n",
    "    axes[row_idx, 1].set_title(f\"{description} - Accuracy\")\n",
    "    axes[row_idx, 1].set_xlabel(\"Epoch\")\n",
    "    axes[row_idx, 1].set_ylabel(\"Accuracy\")\n",
    "    axes[row_idx, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(TRAINING_CURVES_PNG, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved:\", TRAINING_CURVES_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices per case\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, (case, description) in enumerate(cases):\n",
    "    cm = results[case][\"confusion_matrix\"]\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        cbar=False,\n",
    "        xticklabels=[\"Healthy (0)\", \"Anemic (1)\"],\n",
    "        yticklabels=[\"Healthy (0)\", \"Anemic (1)\"],\n",
    "        ax=axes[idx],\n",
    "    )\n",
    "    axes[idx].set_title(description)\n",
    "    axes[idx].set_xlabel(\"Predicted\")\n",
    "    axes[idx].set_ylabel(\"True\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFUSION_MATRICES_PNG, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved:\", CONFUSION_MATRICES_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy/F1 comparison bar chart\n",
    "plot_df = comparison_df[[\"case\", \"accuracy\", \"f1\"]].copy()\n",
    "plot_df = plot_df.melt(id_vars=\"case\", var_name=\"metric\", value_name=\"value\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=plot_df, x=\"case\", y=\"value\", hue=\"metric\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Case Comparison: Accuracy vs F1\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Case\")\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(ACCURACY_F1_PNG, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved:\", ACCURACY_F1_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = [\n",
    "    TRAIN_CSV,\n",
    "    VAL_CSV,\n",
    "    TEST_CSV,\n",
    "    SCRATCH_CKPT,\n",
    "    TRANSFER_CKPT,\n",
    "    COMPARISON_CSV,\n",
    "    TRAINING_CURVES_PNG,\n",
    "    CONFUSION_MATRICES_PNG,\n",
    "    ACCURACY_F1_PNG,\n",
    "]\n",
    "\n",
    "print(\"Artifact check:\")\n",
    "for path in artifacts:\n",
    "    exists = path.exists()\n",
    "    size = path.stat().st_size if exists else 0\n",
    "    print(f\" - {path}: {'OK' if exists else 'MISSING'} (bytes={size})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
