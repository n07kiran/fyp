{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AneRBC-I Binary Classification (Anemic vs Healthy) with VGG16\n",
    "\n",
    "This notebook implements a reproducible TensorFlow/Keras binary classification pipeline using **AneRBC-I `Original_images` only**.\n",
    "\n",
    "## Cases Compared\n",
    "- **Case 1**: VGG16 from scratch (`weights=None`, trainable base)\n",
    "- **Case 2**: Transfer learning (`weights='imagenet'`, frozen base only)\n",
    "\n",
    "## Fixed Split (seed=42)\n",
    "- Train: 700 images (350 Healthy / 350 Anemic)\n",
    "- Validation: 100 images (50 Healthy / 50 Anemic)\n",
    "- Test: 200 images (100 Healthy / 100 Anemic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment (Python 3.11)\n",
    "\n",
    "Use the project virtualenv before running this notebook:\n",
    "\n",
    "```bash\n",
    "cd <project-root>          # e.g. cd ~/Downloads/fyp\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "> The notebook auto-detects the project root at runtime via `find_project_root()` — no hardcoded paths needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d51123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.1\n",
      "Project root: /Users/kiran/Downloads/fyp\n",
      "Artifacts dir: /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "SEED = 42\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "MAX_EPOCHS = 30\n",
    "LABEL_TO_ID = {\"Healthy\": 0, \"Anemic\": 1}\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "try:\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    for candidate in [start] + list(start.parents):\n",
    "        if (candidate / \"AneRBC_dataset\" / \"AneRBC-I\").exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Could not locate project root containing AneRBC_dataset/AneRBC-I\")\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "DATASET_ROOT = PROJECT_ROOT / \"AneRBC_dataset\" / \"AneRBC-I\"\n",
    "ANEMIC_DIR = DATASET_ROOT / \"Anemic_individuals\" / \"Original_images\"\n",
    "HEALTHY_DIR = DATASET_ROOT / \"Healthy_individuals\" / \"Original_images\"\n",
    "\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"Code\" / \"ImageClassification\" / \"artifacts\"\n",
    "SPLITS_DIR = ARTIFACTS_DIR / \"splits\"\n",
    "MODELS_DIR = ARTIFACTS_DIR / \"models\"\n",
    "METRICS_DIR = ARTIFACTS_DIR / \"metrics\"\n",
    "PLOTS_DIR = ARTIFACTS_DIR / \"plots\"\n",
    "\n",
    "for directory in [ARTIFACTS_DIR, SPLITS_DIR, MODELS_DIR, METRICS_DIR, PLOTS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_CSV = SPLITS_DIR / \"train_split.csv\"\n",
    "VAL_CSV = SPLITS_DIR / \"val_split.csv\"\n",
    "TEST_CSV = SPLITS_DIR / \"test_split.csv\"\n",
    "\n",
    "SCRATCH_CKPT = MODELS_DIR / \"vgg16_scratch_best.keras\"\n",
    "TRANSFER_CKPT = MODELS_DIR / \"vgg16_transfer_frozen_best.keras\"\n",
    "\n",
    "COMPARISON_CSV = METRICS_DIR / \"comparison_metrics.csv\"\n",
    "\n",
    "TRAINING_CURVES_PNG = PLOTS_DIR / \"training_curves.png\"\n",
    "CONFUSION_MATRICES_PNG = PLOTS_DIR / \"confusion_matrices.png\"\n",
    "ACCURACY_F1_PNG = PLOTS_DIR / \"accuracy_f1_comparison.png\"\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Artifacts dir:\", ARTIFACTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4884fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_EXTENSIONS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "\n",
    "def _collect_image_paths(folder: Path) -> list[Path]:\n",
    "    return sorted(\n",
    "        [p for p in folder.iterdir() if p.is_file() and p.suffix.lower() in VALID_EXTENSIONS],\n",
    "        key=lambda p: p.name,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_dataframe(anemic_dir: str, healthy_dir: str) -> pd.DataFrame:\n",
    "    anemic_paths = _collect_image_paths(Path(anemic_dir))\n",
    "    healthy_paths = _collect_image_paths(Path(healthy_dir))\n",
    "\n",
    "    rows = []\n",
    "    for path in healthy_paths:\n",
    "        rows.append({\"filepath\": str(path.resolve().relative_to(PROJECT_ROOT)), \"label\": \"Healthy\"})\n",
    "    for path in anemic_paths:\n",
    "        rows.append({\"filepath\": str(path.resolve().relative_to(PROJECT_ROOT)), \"label\": \"Anemic\"})\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"filepath\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def resolve_paths(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return a copy of df with filepaths resolved to absolute paths from PROJECT_ROOT.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"filepath\"] = df[\"filepath\"].apply(lambda p: str(PROJECT_ROOT / p))\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_splits(df: pd.DataFrame, seed: int = 42) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    stratify_labels = df[\"label\"].map(LABEL_TO_ID)\n",
    "\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.30,\n",
    "        random_state=seed,\n",
    "        shuffle=True,\n",
    "        stratify=stratify_labels,\n",
    "    )\n",
    "\n",
    "    temp_stratify_labels = temp_df[\"label\"].map(LABEL_TO_ID)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=2 / 3,\n",
    "        random_state=seed,\n",
    "        shuffle=True,\n",
    "        stratify=temp_stratify_labels,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train_df.reset_index(drop=True),\n",
    "        val_df.reset_index(drop=True),\n",
    "        test_df.reset_index(drop=True),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700aa201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split files:\n",
      " - /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/splits/train_split.csv\n",
      " - /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/splits/val_split.csv\n",
      " - /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/splits/test_split.csv\n",
      "\n",
      "Split summary:\n",
      "   split  count\n",
      "0  train    700\n",
      "1    val    100\n",
      "2   test    200\n",
      "\n",
      "Per-class counts:\n",
      "train\n",
      "label\n",
      "Anemic     350\n",
      "Healthy    350\n",
      "Name: count, dtype: int64\n",
      "\n",
      "val\n",
      "label\n",
      "Anemic     50\n",
      "Healthy    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "test\n",
      "label\n",
      "Anemic     100\n",
      "Healthy    100\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def assert_split_integrity(train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> None:\n",
    "    # Exact sample counts\n",
    "    assert len(train_df) == 700, f\"Expected 700 train samples, found {len(train_df)}\"\n",
    "    assert len(val_df) == 100, f\"Expected 100 val samples, found {len(val_df)}\"\n",
    "    assert len(test_df) == 200, f\"Expected 200 test samples, found {len(test_df)}\"\n",
    "\n",
    "    # Exact per-class counts\n",
    "    expected_class_counts = {\n",
    "        \"train\": {\"Healthy\": 350, \"Anemic\": 350},\n",
    "        \"val\": {\"Healthy\": 50, \"Anemic\": 50},\n",
    "        \"test\": {\"Healthy\": 100, \"Anemic\": 100},\n",
    "    }\n",
    "\n",
    "    split_map = {\"train\": train_df, \"val\": val_df, \"test\": test_df}\n",
    "    for split_name, split_df in split_map.items():\n",
    "        counts = split_df[\"label\"].value_counts().to_dict()\n",
    "        assert counts == expected_class_counts[split_name], (\n",
    "            f\"{split_name} class counts mismatch. Expected {expected_class_counts[split_name]}, got {counts}\"\n",
    "        )\n",
    "\n",
    "    # Zero overlap among filepaths (compare as relative paths)\n",
    "    train_paths = set(train_df[\"filepath\"])\n",
    "    val_paths = set(val_df[\"filepath\"])\n",
    "    test_paths = set(test_df[\"filepath\"])\n",
    "\n",
    "    assert train_paths.isdisjoint(val_paths), \"Leakage detected between train and val\"\n",
    "    assert train_paths.isdisjoint(test_paths), \"Leakage detected between train and test\"\n",
    "    assert val_paths.isdisjoint(test_paths), \"Leakage detected between val and test\"\n",
    "\n",
    "    # All files exist and are readable (resolve relative paths against PROJECT_ROOT)\n",
    "    all_paths = train_paths | val_paths | test_paths\n",
    "    bad_paths = [\n",
    "        path for path in all_paths\n",
    "        if not (PROJECT_ROOT / path).is_file() or not os.access(str(PROJECT_ROOT / path), os.R_OK)\n",
    "    ]\n",
    "    assert not bad_paths, f\"Found missing/unreadable files: {len(bad_paths)}\"\n",
    "\n",
    "\n",
    "full_df = build_dataframe(str(ANEMIC_DIR), str(HEALTHY_DIR))\n",
    "assert full_df.shape[0] == 1000, f\"Expected 1000 rows, found {full_df.shape[0]}\"\n",
    "\n",
    "train_df, val_df, test_df = make_splits(full_df, seed=SEED)\n",
    "assert_split_integrity(train_df, val_df, test_df)\n",
    "\n",
    "# Reproducibility check: rerunning with the same seed yields identical split memberships\n",
    "train_df_2, val_df_2, test_df_2 = make_splits(full_df, seed=SEED)\n",
    "assert set(train_df[\"filepath\"]) == set(train_df_2[\"filepath\"])\n",
    "assert set(val_df[\"filepath\"]) == set(val_df_2[\"filepath\"])\n",
    "assert set(test_df[\"filepath\"]) == set(test_df_2[\"filepath\"])\n",
    "\n",
    "# Stable output ordering\n",
    "train_df = train_df.sort_values(\"filepath\").reset_index(drop=True)\n",
    "val_df = val_df.sort_values(\"filepath\").reset_index(drop=True)\n",
    "test_df = test_df.sort_values(\"filepath\").reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv(TRAIN_CSV, index=False)\n",
    "val_df.to_csv(VAL_CSV, index=False)\n",
    "test_df.to_csv(TEST_CSV, index=False)\n",
    "\n",
    "print(\"Saved split files:\")\n",
    "print(\" -\", TRAIN_CSV)\n",
    "print(\" -\", VAL_CSV)\n",
    "print(\" -\", TEST_CSV)\n",
    "\n",
    "print(\"\\nSplit summary:\")\n",
    "summary_df = pd.DataFrame({\n",
    "    \"split\": [\"train\", \"val\", \"test\"],\n",
    "    \"count\": [len(train_df), len(val_df), len(test_df)],\n",
    "})\n",
    "print(summary_df)\n",
    "\n",
    "print(\"\\nPer-class counts:\")\n",
    "for split_name, split_df in {\"train\": train_df, \"val\": val_df, \"test\": test_df}.items():\n",
    "    print(split_name)\n",
    "    print(split_df[\"label\"].value_counts().sort_index())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f64e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 700 validated image filenames belonging to 2 classes.\n",
      "Found 100 validated image filenames belonging to 2 classes.\n",
      "Found 200 validated image filenames belonging to 2 classes.\n",
      "Generator checks passed.\n",
      "Class indices: {'Healthy': 0, 'Anemic': 1}\n",
      "Sample batch shape: (16, 224, 224, 3) Label shape: (16,)\n",
      "Pixel range: 0.04313725605607033 to 0.9568628072738647\n"
     ]
    }
   ],
   "source": [
    "def make_generators(train_df, val_df, test_df, image_size=(224, 224), batch_size=16):\n",
    "    classes = [\"Healthy\", \"Anemic\"]\n",
    "\n",
    "    # Resolve relative paths to absolute before passing to Keras generators\n",
    "    train_df_abs = resolve_paths(train_df)\n",
    "    val_df_abs = resolve_paths(val_df)\n",
    "    test_df_abs = resolve_paths(test_df)\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "    eval_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "    common_kwargs = {\n",
    "        \"x_col\": \"filepath\",\n",
    "        \"y_col\": \"label\",\n",
    "        \"target_size\": image_size,\n",
    "        \"color_mode\": \"rgb\",\n",
    "        \"class_mode\": \"binary\",\n",
    "        \"classes\": classes,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"validate_filenames\": True,\n",
    "    }\n",
    "\n",
    "    train_gen = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df_abs,\n",
    "        shuffle=True,\n",
    "        seed=SEED,\n",
    "        **common_kwargs,\n",
    "    )\n",
    "\n",
    "    val_gen = eval_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df_abs,\n",
    "        shuffle=False,\n",
    "        **common_kwargs,\n",
    "    )\n",
    "\n",
    "    test_gen = eval_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df_abs,\n",
    "        shuffle=False,\n",
    "        **common_kwargs,\n",
    "    )\n",
    "\n",
    "    return train_gen, val_gen, test_gen\n",
    "\n",
    "\n",
    "train_gen, val_gen, test_gen = make_generators(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    test_df,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Label encoding test\n",
    "expected_class_indices = {\"Healthy\": 0, \"Anemic\": 1}\n",
    "assert train_gen.class_indices == expected_class_indices, (\n",
    "    f\"Class indices mismatch. Expected {expected_class_indices}, got {train_gen.class_indices}\"\n",
    ")\n",
    "\n",
    "# Input shape/range test\n",
    "x_batch, y_batch = next(train_gen)\n",
    "assert x_batch.shape[1:] == (224, 224, 3), f\"Unexpected input shape: {x_batch.shape}\"\n",
    "assert np.min(x_batch) >= 0.0 and np.max(x_batch) <= 1.0, \"Input pixel range should be [0, 1]\"\n",
    "assert y_batch.ndim == 1, f\"Expected binary labels as rank-1 vector, got shape {y_batch.shape}\"\n",
    "train_gen.reset()\n",
    "\n",
    "print(\"Generator checks passed.\")\n",
    "print(\"Class indices:\", train_gen.class_indices)\n",
    "print(\"Sample batch shape:\", x_batch.shape, \"Label shape:\", y_batch.shape)\n",
    "print(\"Pixel range:\", float(np.min(x_batch)), \"to\", float(np.max(x_batch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "487f0308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration checks passed.\n",
      "Case 1 base trainable: True\n",
      "Case 2 base trainable: False\n"
     ]
    }
   ],
   "source": [
    "def build_vgg16_binary(case: str) -> tf.keras.Model:\n",
    "    if case not in {\"scratch\", \"transfer_frozen\"}:\n",
    "        raise ValueError(\"case must be one of {'scratch', 'transfer_frozen'}\")\n",
    "\n",
    "    weights = None if case == \"scratch\" else \"imagenet\"\n",
    "    base = VGG16(\n",
    "        weights=weights,\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3),\n",
    "    )\n",
    "    base.trainable = case == \"scratch\"\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    x = base(inputs, training=base.trainable)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=f\"vgg16_{case}_binary\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Model configuration tests\n",
    "scratch_model = build_vgg16_binary(\"scratch\")\n",
    "transfer_model = build_vgg16_binary(\"transfer_frozen\")\n",
    "\n",
    "assert scratch_model.get_layer(\"vgg16\").trainable is True, \"Case 1 base should be trainable\"\n",
    "assert transfer_model.get_layer(\"vgg16\").trainable is False, \"Case 2 base should be frozen\"\n",
    "\n",
    "print(\"Model configuration checks passed.\")\n",
    "print(\"Case 1 base trainable:\", scratch_model.get_layer(\"vgg16\").trainable)\n",
    "print(\"Case 2 base trainable:\", transfer_model.get_layer(\"vgg16\").trainable)\n",
    "\n",
    "del scratch_model, transfer_model\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab29293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Case 1: VGG16 without transfer learning\n",
      "Epoch 1/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.4660 - loss: 0.6936\n",
      "Epoch 1: val_accuracy improved from None to 0.50000, saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_scratch_best.keras\n",
      "\n",
      "Epoch 1: finished saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_scratch_best.keras\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 4s/step - accuracy: 0.4543 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 2/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.5396 - loss: 0.6931\n",
      "Epoch 2: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 6s/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 3/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.4902 - loss: 0.6932\n",
      "Epoch 3: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 6s/step - accuracy: 0.5071 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 4/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5138 - loss: 0.6932\n",
      "Epoch 4: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 6s/step - accuracy: 0.4986 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 5/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.4674 - loss: 0.6931\n",
      "Epoch 5: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 6s/step - accuracy: 0.4700 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 6/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5292 - loss: 0.6932\n",
      "Epoch 6: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 6s/step - accuracy: 0.4843 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Saved best checkpoint to: /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_scratch_best.keras\n",
      "\n",
      "================================================================================\n",
      "Case 2: VGG16 transfer learning (frozen base)\n",
      "Epoch 1/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4918 - loss: 0.7504\n",
      "Epoch 1: val_accuracy improved from None to 0.65000, saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\n",
      "Epoch 1: finished saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 2s/step - accuracy: 0.5100 - loss: 0.7302 - val_accuracy: 0.6500 - val_loss: 0.6719\n",
      "Epoch 2/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5601 - loss: 0.7145\n",
      "Epoch 2: val_accuracy improved from 0.65000 to 0.71000, saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\n",
      "Epoch 2: finished saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 3s/step - accuracy: 0.5743 - loss: 0.6983 - val_accuracy: 0.7100 - val_loss: 0.6533\n",
      "Epoch 3/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5785 - loss: 0.6583\n",
      "Epoch 3: val_accuracy did not improve from 0.71000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 3s/step - accuracy: 0.5714 - loss: 0.6665 - val_accuracy: 0.7100 - val_loss: 0.6363\n",
      "Epoch 4/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5594 - loss: 0.6760\n",
      "Epoch 4: val_accuracy improved from 0.71000 to 0.73000, saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\n",
      "Epoch 4: finished saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 3s/step - accuracy: 0.5771 - loss: 0.6787 - val_accuracy: 0.7300 - val_loss: 0.6241\n",
      "Epoch 5/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6259 - loss: 0.6482\n",
      "Epoch 5: val_accuracy did not improve from 0.73000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 3s/step - accuracy: 0.6157 - loss: 0.6601 - val_accuracy: 0.6900 - val_loss: 0.6201\n",
      "Epoch 6/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5971 - loss: 0.6561\n",
      "Epoch 6: val_accuracy did not improve from 0.73000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 3s/step - accuracy: 0.5957 - loss: 0.6604 - val_accuracy: 0.7100 - val_loss: 0.6055\n",
      "Epoch 7/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6576 - loss: 0.6317\n",
      "Epoch 7: val_accuracy improved from 0.73000 to 0.74000, saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\n",
      "Epoch 7: finished saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 3s/step - accuracy: 0.6271 - loss: 0.6424 - val_accuracy: 0.7400 - val_loss: 0.5984\n",
      "Epoch 8/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6456 - loss: 0.6441\n",
      "Epoch 8: val_accuracy did not improve from 0.74000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 3s/step - accuracy: 0.6186 - loss: 0.6544 - val_accuracy: 0.7400 - val_loss: 0.5893\n",
      "Epoch 9/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6363 - loss: 0.6250\n",
      "Epoch 9: val_accuracy did not improve from 0.74000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 3s/step - accuracy: 0.6300 - loss: 0.6289 - val_accuracy: 0.7400 - val_loss: 0.5839\n",
      "Epoch 10/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6364 - loss: 0.6409\n",
      "Epoch 10: val_accuracy improved from 0.74000 to 0.76000, saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\n",
      "Epoch 10: finished saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.6543 - loss: 0.6270 - val_accuracy: 0.7600 - val_loss: 0.5784\n",
      "Epoch 11/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6950 - loss: 0.6138\n",
      "Epoch 11: val_accuracy did not improve from 0.76000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - accuracy: 0.6800 - loss: 0.6220 - val_accuracy: 0.7600 - val_loss: 0.5726\n",
      "Epoch 12/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6585 - loss: 0.6223\n",
      "Epoch 12: val_accuracy did not improve from 0.76000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.6429 - loss: 0.6290 - val_accuracy: 0.7600 - val_loss: 0.5680\n",
      "Epoch 13/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6388 - loss: 0.6187\n",
      "Epoch 13: val_accuracy did not improve from 0.76000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.6571 - loss: 0.6031 - val_accuracy: 0.7500 - val_loss: 0.5648\n",
      "Epoch 14/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6655 - loss: 0.6175\n",
      "Epoch 14: val_accuracy improved from 0.76000 to 0.78000, saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\n",
      "Epoch 14: finished saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.6671 - loss: 0.6123 - val_accuracy: 0.7800 - val_loss: 0.5611\n",
      "Epoch 15/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6784 - loss: 0.5926\n",
      "Epoch 15: val_accuracy did not improve from 0.78000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - accuracy: 0.6729 - loss: 0.6034 - val_accuracy: 0.7600 - val_loss: 0.5553\n",
      "Epoch 16/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6970 - loss: 0.6026\n",
      "Epoch 16: val_accuracy did not improve from 0.78000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - accuracy: 0.6943 - loss: 0.5946 - val_accuracy: 0.7600 - val_loss: 0.5516\n",
      "Epoch 17/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6832 - loss: 0.5784\n",
      "Epoch 17: val_accuracy did not improve from 0.78000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - accuracy: 0.6814 - loss: 0.5845 - val_accuracy: 0.7700 - val_loss: 0.5504\n",
      "Epoch 18/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6659 - loss: 0.6153\n",
      "Epoch 18: val_accuracy did not improve from 0.78000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.6871 - loss: 0.5991 - val_accuracy: 0.7600 - val_loss: 0.5433\n",
      "Epoch 19/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7191 - loss: 0.5684\n",
      "Epoch 19: val_accuracy did not improve from 0.78000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.7014 - loss: 0.5853 - val_accuracy: 0.7600 - val_loss: 0.5394\n",
      "Epoch 20/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6664 - loss: 0.6003\n",
      "Epoch 20: val_accuracy did not improve from 0.78000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.6729 - loss: 0.5862 - val_accuracy: 0.7400 - val_loss: 0.5372\n",
      "Epoch 21/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7007 - loss: 0.5688\n",
      "Epoch 21: val_accuracy improved from 0.78000 to 0.79000, saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\n",
      "Epoch 21: finished saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.6829 - loss: 0.5805 - val_accuracy: 0.7900 - val_loss: 0.5325\n",
      "Epoch 22/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7318 - loss: 0.5572\n",
      "Epoch 22: val_accuracy did not improve from 0.79000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.6971 - loss: 0.5773 - val_accuracy: 0.7900 - val_loss: 0.5287\n",
      "Epoch 23/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6743 - loss: 0.5723\n",
      "Epoch 23: val_accuracy did not improve from 0.79000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.6900 - loss: 0.5731 - val_accuracy: 0.7400 - val_loss: 0.5274\n",
      "Epoch 24/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7326 - loss: 0.5539\n",
      "Epoch 24: val_accuracy did not improve from 0.79000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - accuracy: 0.7129 - loss: 0.5649 - val_accuracy: 0.7700 - val_loss: 0.5238\n",
      "Epoch 25/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7138 - loss: 0.5641\n",
      "Epoch 25: val_accuracy did not improve from 0.79000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.7086 - loss: 0.5581 - val_accuracy: 0.7900 - val_loss: 0.5202\n",
      "Epoch 26/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7046 - loss: 0.5774\n",
      "Epoch 26: val_accuracy did not improve from 0.79000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.7029 - loss: 0.5643 - val_accuracy: 0.7800 - val_loss: 0.5180\n",
      "Epoch 27/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6670 - loss: 0.5750\n",
      "Epoch 27: val_accuracy did not improve from 0.79000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.7014 - loss: 0.5504 - val_accuracy: 0.7800 - val_loss: 0.5142\n",
      "Epoch 28/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7153 - loss: 0.5371\n",
      "Epoch 28: val_accuracy did not improve from 0.79000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.7057 - loss: 0.5553 - val_accuracy: 0.7900 - val_loss: 0.5119\n",
      "Epoch 29/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7372 - loss: 0.5546\n",
      "Epoch 29: val_accuracy improved from 0.79000 to 0.80000, saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\n",
      "Epoch 29: finished saving model to /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.7229 - loss: 0.5489 - val_accuracy: 0.8000 - val_loss: 0.5107\n",
      "Epoch 30/30\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7304 - loss: 0.5385\n",
      "Epoch 30: val_accuracy did not improve from 0.80000\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.7114 - loss: 0.5462 - val_accuracy: 0.8000 - val_loss: 0.5070\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "Saved best checkpoint to: /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_transfer_frozen_best.keras\n"
     ]
    }
   ],
   "source": [
    "def train_case(case: str, train_gen, val_gen, epochs: int = 30):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_vgg16_binary(case)\n",
    "\n",
    "    checkpoint_path = SCRATCH_CKPT if case == \"scratch\" else TRANSFER_CKPT\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(checkpoint_path),\n",
    "            monitor=\"val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return model, history, checkpoint_path\n",
    "\n",
    "\n",
    "cases = [\n",
    "    (\"scratch\", \"Case 1: VGG16 without transfer learning\"),\n",
    "    (\"transfer_frozen\", \"Case 2: VGG16 transfer learning (frozen base)\"),\n",
    "]\n",
    "\n",
    "trained_models = {}\n",
    "histories = {}\n",
    "\n",
    "for case, description in cases:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(description)\n",
    "    model, history, checkpoint_path = train_case(case, train_gen, val_gen, epochs=MAX_EPOCHS)\n",
    "    trained_models[case] = {\"model\": model, \"checkpoint\": checkpoint_path}\n",
    "    histories[case] = history.history\n",
    "    print(f\"Saved best checkpoint to: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict_labels(model: tf.keras.Model, test_gen):\n",
    "    test_gen.reset()\n",
    "    y_true = test_gen.classes.astype(int)\n",
    "    y_prob = model.predict(test_gen, verbose=0).ravel()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    return y_true, y_pred, y_prob\n",
    "\n",
    "\n",
    "def evaluate_case(model, test_gen) -> dict[str, float]:\n",
    "    y_true, y_pred, _ = _predict_labels(model, test_gen)\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, pos_label=1)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, pos_label=1, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, pos_label=1, zero_division=0)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Checkpoint not found: /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_scratch_best.keras",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m checkpoint_path = SCRATCH_CKPT \u001b[38;5;28;01mif\u001b[39;00m case == \u001b[33m\"\u001b[39m\u001b[33mscratch\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m TRANSFER_CKPT\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checkpoint_path.exists():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m best_model = tf.keras.models.load_model(checkpoint_path)\n\u001b[32m      9\u001b[39m metrics = evaluate_case(best_model, test_gen)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Checkpoint not found: /Users/kiran/Downloads/fyp/Code/ImageClassification/artifacts/models/vgg16_scratch_best.keras"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for case, description in cases:\n",
    "    checkpoint_path = SCRATCH_CKPT if case == \"scratch\" else TRANSFER_CKPT\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "    best_model = tf.keras.models.load_model(checkpoint_path)\n",
    "    metrics = evaluate_case(best_model, test_gen)\n",
    "    y_true, y_pred, _ = _predict_labels(best_model, test_gen)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=[0, 1],\n",
    "        target_names=[\"Healthy\", \"Anemic\"],\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    # Evaluation tests\n",
    "    assert cm.shape == (2, 2), f\"Confusion matrix for {case} must be 2x2, got {cm.shape}\"\n",
    "    assert np.all(np.isfinite(list(metrics.values()))), f\"Non-finite metric values detected for {case}\"\n",
    "\n",
    "    results[case] = {\n",
    "        **metrics,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": report,\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(description)\n",
    "    print(\"Test metrics:\")\n",
    "    print({k: round(v, 4) for k, v in metrics.items()})\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'scratch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      6\u001b[39m comparison_rows = []\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m case, _ \u001b[38;5;129;01min\u001b[39;00m cases:\n\u001b[32m      8\u001b[39m     comparison_rows.append(\n\u001b[32m      9\u001b[39m         {\n\u001b[32m     10\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcase\u001b[39m\u001b[33m\"\u001b[39m: case_name_map[case],\n\u001b[32m     11\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m: case,\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcase\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     13\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: results[case][\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     14\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: results[case][\u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     15\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m: results[case][\u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     16\u001b[39m         }\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     19\u001b[39m comparison_df = pd.DataFrame(comparison_rows)\n\u001b[32m     20\u001b[39m comparison_df = comparison_df.sort_values(by=[\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m], ascending=[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyError\u001b[39m: 'scratch'"
     ]
    }
   ],
   "source": [
    "case_name_map = {\n",
    "    \"scratch\": \"Case 1\",\n",
    "    \"transfer_frozen\": \"Case 2\",\n",
    "}\n",
    "\n",
    "comparison_rows = []\n",
    "for case, _ in cases:\n",
    "    comparison_rows.append(\n",
    "        {\n",
    "            \"case\": case_name_map[case],\n",
    "            \"config\": case,\n",
    "            \"accuracy\": results[case][\"accuracy\"],\n",
    "            \"f1\": results[case][\"f1\"],\n",
    "            \"precision\": results[case][\"precision\"],\n",
    "            \"recall\": results[case][\"recall\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "comparison_df = comparison_df.sort_values(by=[\"accuracy\", \"f1\"], ascending=[False, False]).reset_index(drop=True)\n",
    "comparison_df.insert(0, \"rank\", np.arange(1, len(comparison_df) + 1))\n",
    "\n",
    "comparison_df.to_csv(COMPARISON_CSV, index=False)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves per case\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for row_idx, (case, description) in enumerate(cases):\n",
    "    history = histories[case]\n",
    "\n",
    "    axes[row_idx, 0].plot(history.get(\"loss\", []), label=\"train_loss\")\n",
    "    axes[row_idx, 0].plot(history.get(\"val_loss\", []), label=\"val_loss\")\n",
    "    axes[row_idx, 0].set_title(f\"{description} - Loss\")\n",
    "    axes[row_idx, 0].set_xlabel(\"Epoch\")\n",
    "    axes[row_idx, 0].set_ylabel(\"Loss\")\n",
    "    axes[row_idx, 0].legend()\n",
    "\n",
    "    axes[row_idx, 1].plot(history.get(\"accuracy\", []), label=\"train_accuracy\")\n",
    "    axes[row_idx, 1].plot(history.get(\"val_accuracy\", []), label=\"val_accuracy\")\n",
    "    axes[row_idx, 1].set_title(f\"{description} - Accuracy\")\n",
    "    axes[row_idx, 1].set_xlabel(\"Epoch\")\n",
    "    axes[row_idx, 1].set_ylabel(\"Accuracy\")\n",
    "    axes[row_idx, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(TRAINING_CURVES_PNG, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved:\", TRAINING_CURVES_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices per case\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, (case, description) in enumerate(cases):\n",
    "    cm = results[case][\"confusion_matrix\"]\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        cbar=False,\n",
    "        xticklabels=[\"Healthy (0)\", \"Anemic (1)\"],\n",
    "        yticklabels=[\"Healthy (0)\", \"Anemic (1)\"],\n",
    "        ax=axes[idx],\n",
    "    )\n",
    "    axes[idx].set_title(description)\n",
    "    axes[idx].set_xlabel(\"Predicted\")\n",
    "    axes[idx].set_ylabel(\"True\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFUSION_MATRICES_PNG, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved:\", CONFUSION_MATRICES_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy/F1 comparison bar chart\n",
    "plot_df = comparison_df[[\"case\", \"accuracy\", \"f1\"]].copy()\n",
    "plot_df = plot_df.melt(id_vars=\"case\", var_name=\"metric\", value_name=\"value\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=plot_df, x=\"case\", y=\"value\", hue=\"metric\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Case Comparison: Accuracy vs F1\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Case\")\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(ACCURACY_F1_PNG, dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved:\", ACCURACY_F1_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = [\n",
    "    TRAIN_CSV,\n",
    "    VAL_CSV,\n",
    "    TEST_CSV,\n",
    "    SCRATCH_CKPT,\n",
    "    TRANSFER_CKPT,\n",
    "    COMPARISON_CSV,\n",
    "    TRAINING_CURVES_PNG,\n",
    "    CONFUSION_MATRICES_PNG,\n",
    "    ACCURACY_F1_PNG,\n",
    "]\n",
    "\n",
    "print(\"Artifact check:\")\n",
    "for path in artifacts:\n",
    "    exists = path.exists()\n",
    "    size = path.stat().st_size if exists else 0\n",
    "    print(f\" - {path}: {'OK' if exists else 'MISSING'} (bytes={size})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
